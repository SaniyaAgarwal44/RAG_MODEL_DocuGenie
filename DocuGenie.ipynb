{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022f8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6e80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ff38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c524f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6284\\966817116.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI()\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6284\\966817116.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = HuggingFaceEmbeddings(model_name = \"BAAI/bge-large-en-v1.5\",model_kwargs={'device': 'cpu'},encode_kwargs={\"normalize_embeddings\": True})\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI()\n",
    "embedding_function = HuggingFaceEmbeddings(model_name = \"BAAI/bge-large-en-v1.5\",model_kwargs={'device': 'cpu'},encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56444e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b83098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cce95822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_docs(path):\n",
    "    loader = PyPDFLoader(file_path=path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "    \n",
    "                                                                                                \n",
    "    db = FAISS.from_documents(documents=docs, embedding=embedding_function)\n",
    "    return db\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692ddb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def answer_query(message, chat_history):\\n    base_compressor = LLMChainExtractor.from_llm(chat)\\n    base_retriever = db.as_retriever()\\n    mq_retriever = MultiQueryRetriever.from_llm(retriever = base_retriever, llm=chat)\\n    compression_retriever = ContextualCompressionRetriever(base_compressor=base_compressor, base_retriever=mq_retriever)\\n\\n    #matched_docs = compression_retriever.get_relevant_documents(query = message)---->old version\\n    \\n    \\n    matched_docs = compression_retriever.invoke(message)\\n\\n    context = \"\"\\n\\n    for doc in matched_docs:\\n        page_content = doc.page_content\\n        context+=page_content\\n        context += \"\\n\\n\"\\n    template = \"\"\"\\n    Answer the following question only by using the context given below in the triple backticks, do not use any other information to answer the question.\\n    If you can\\'t answer the given question with the given context, you can return an emtpy string (\\'\\')\\n    Context: ```{context}```\\n    ----------------------------\\n    Question: {query}\\n    ----------------------------\\n    Answer: \"\"\"\\n    \\n    human_message_prompt = HumanMessagePromptTemplate.from_template(template=template)\\n    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\\n    prompt = chat_prompt.format_prompt(query = message, context = context)\\n    #response = chat(messages=prompt.to_messages()).content---->old method\\n    \\n    \\n    #Updated method: invoke() instead of chat(messages=...)\\n    response = chat.invoke(prompt.to_messages()).content\\n\\n    chat_history.append((message,response))\\n    return \"\", chat_history'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def answer_query(message, chat_history):\n",
    "    base_compressor = LLMChainExtractor.from_llm(chat)\n",
    "    base_retriever = db.as_retriever()\n",
    "    mq_retriever = MultiQueryRetriever.from_llm(retriever = base_retriever, llm=chat)\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=base_compressor, base_retriever=mq_retriever)\n",
    "\n",
    "    #matched_docs = compression_retriever.get_relevant_documents(query = message)---->old version\n",
    "    \n",
    "    \n",
    "    matched_docs = compression_retriever.invoke(message)\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in matched_docs:\n",
    "        page_content = doc.page_content\n",
    "        context+=page_content\n",
    "        context += \"\\n\\n\"\n",
    "    template = \"\"\"\n",
    "    Answer the following question only by using the context given below in the triple backticks, do not use any other information to answer the question.\n",
    "    If you can't answer the given question with the given context, you can return an emtpy string ('')\n",
    "    Context: ```{context}```\n",
    "    ----------------------------\n",
    "    Question: {query}\n",
    "    ----------------------------\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(template=template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "    prompt = chat_prompt.format_prompt(query = message, context = context)\n",
    "    #response = chat(messages=prompt.to_messages()).content---->old method\n",
    "    \n",
    "    \n",
    "    #Updated method: invoke() instead of chat(messages=...)\n",
    "    response = chat.invoke(prompt.to_messages()).content\n",
    "\n",
    "    chat_history.append((message,response))\n",
    "    return \"\", chat_history'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42d42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''def answer_query(message, chat_history):\n",
    "    # Create retrievers and compression retriever\n",
    "    base_compressor = LLMChainExtractor.from_llm(chat)\n",
    "    base_retriever = db.as_retriever()\n",
    "    mq_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=chat)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=base_compressor, base_retriever=mq_retriever\n",
    "    )\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    matched_docs = compression_retriever.invoke(message)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in matched_docs)  # Simplified concatenation\n",
    "\n",
    "    # Define the prompt\n",
    "    template = \"\"\"Answer using only the given context. If the context is insufficient, return an empty string ('').\n",
    "    Context: ```{context}```\n",
    "    ----------------------------\n",
    "    Question: {query}\n",
    "    ----------------------------\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    # Format the prompt and get response\n",
    "    chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "    prompt = chat_prompt.format_prompt(query=message, context=context)\n",
    "    response = chat.invoke(prompt.to_messages()).content  # Updated method\n",
    "\n",
    "    # Append to chat history and return response\n",
    "    chat_history.append((message, response))\n",
    "    return response, chat_history'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d34f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(message, chat_history):\n",
    "    \n",
    "    base_compressor = LLMChainExtractor.from_llm(chat)\n",
    "    base_retriever = db.as_retriever()\n",
    "    mq_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=chat)\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=base_compressor, base_retriever=mq_retriever)\n",
    "\n",
    "    matched_docs = compression_retriever.invoke(message)\n",
    "\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in matched_docs)\n",
    "\n",
    "    template = \"\"\"\n",
    "    Answer the following question only by using the context given below in the triple backticks, do not use any other information to answer the question.\n",
    "    If you can't answer the given question with the given context, return an empty string ('').\n",
    "    Context: ```{context}```\n",
    "    ----------------------------\n",
    "    Question: {query}\n",
    "    ----------------------------\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(template=template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "    prompt = chat_prompt.format_prompt(query = message, context = context)\n",
    "    response = chat.invoke(prompt.to_messages()).content\n",
    "\n",
    "    # Fix: Store messages as dictionaries\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    return chat_history  # Return updated chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the document first\n",
    "db = add_docs(\"Atomic habits.pdf\")  # Ensure the PDF exists and is correct\n",
    "\n",
    "# Then use db in answer_query()\n",
    "message = \"What are some atomic habits we should follow?\"\n",
    "\n",
    "output= answer_query(message, chat_history, db)\n",
    "\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578350e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (2164883569.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    message = \"What are the some atomic habits we should follow?import gradio as gr\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''''import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"<h1 align='center'>Smart Assistant</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        upload_files = gr.File(label=\"Upload a PDF\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Enter your question here\")\n",
    "\n",
    "    # Ensure document is uploaded before answering questions\n",
    "    upload_files.upload(fn=add_docs, inputs=upload_files, outputs=None)\n",
    "\n",
    "    # Submitting a question should process it and update chatbot\n",
    "    msg.submit(fn=answer_query, inputs=[msg, chatbot], outputs=[chatbot])\n",
    "\n",
    "# Launch UI\n",
    "demo.launch()\n",
    "\"\n",
    "chat_history = []  # Initialize an empty chat history\n",
    "\n",
    "output, chat_history = answer_query(message, chat_history)\n",
    "\n",
    "# Print the response\n",
    "print(output)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4297c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"<h1 align='center'>DocuGenie By Saniya</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        upload_files = gr.File(label=\"Upload a PDF\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "\n",
    "    chatbot = gr.Chatbot(type=\"messages\")  #Fix: Use 'messages' type\n",
    "    msg = gr.Textbox(label=\"Enter your question here\")\n",
    "\n",
    "    upload_files.upload(fn=add_docs, inputs=upload_files, outputs=None)\n",
    "    msg.submit(fn=answer_query, inputs=[msg, chatbot], outputs=chatbot)  # ‚úÖ Fix: No need to pass msg back\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b52674d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py:1821: UserWarning: A function (add_docs) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n",
      "    Output components:\n",
      "        []\n",
      "    Output values returned:\n",
      "        [<langchain_community.vectorstores.faiss.FAISS object at 0x0000027A068A3C10>]\n",
      "  warnings.warn(\n",
      "D:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py:1821: UserWarning: A function (add_docs) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n",
      "    Output components:\n",
      "        []\n",
      "    Output values returned:\n",
      "        [<langchain_community.vectorstores.faiss.FAISS object at 0x0000027A502DEE10>]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"<h1 align='center'>DocuGenie By Saniya üßû‚Äç‚ôÇÔ∏è</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        upload_files = gr.File(label=\"Upload a PDFüìÑ\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"üí° AI Assistant\", type=\"messages\")  #Fix: Use 'messages' type\n",
    "    msg = gr.Textbox(label=\"Enter your question here\")\n",
    "\n",
    "    upload_files.upload(fn=add_docs, inputs=upload_files, outputs=None)\n",
    "    msg.submit(fn=answer_query, inputs=[msg, chatbot], outputs=chatbot)  # ‚úÖ Fix: No need to pass msg back\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a6169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py:1821: UserWarning: A function (add_docs) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n",
      "    Output components:\n",
      "        []\n",
      "    Output values returned:\n",
      "        [<langchain_community.vectorstores.faiss.FAISS object at 0x0000027A502DEE10>]\n",
      "  warnings.warn(\n",
      "D:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py:1821: UserWarning: A function (add_docs) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n",
      "    Output components:\n",
      "        []\n",
      "    Output values returned:\n",
      "        [<langchain_community.vectorstores.faiss.FAISS object at 0x0000027A50364D90>]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fd9a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe4955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e44483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cad95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3008a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8503e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc903e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a80dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157960c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e785c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc7433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8bee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75402a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68195b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f6f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747fcda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c6631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea9207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4fa09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e33dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c26581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa047f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fc129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e047b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f42ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc87097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262afa92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b2312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec8b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
